---
title: 'Reduciendo hasta 4x los costos de bases de datos vectoriales con QDrant y surus'
author:
  name: 'Marian Basti'
  avatar: 'https://avatars.githubusercontent.com/u/31198560'
  bio: 'CTO de surus'
tags: ['qdrant', 'vector-database', 'legal-ai', 'argentina', 'embeddings', 'surus']
featured: true
emoji: '丘뒲잺'
---

En el post de hoy vamos a explorar c칩mo aprovechar la API de surus para generar embeddings de dimensiones variables de nuestros textos y almacenarlos en Qdrant, una base de datos vectorial de c칩digo abierto.

Si sale bien, podremos realizar b칰squedas sem치nticas eficientes sobre grandes vol칰menes de documentos.

Como caso de aplicaci칩n, usaremos el Bolet칤n Oficial de Argentina (hosteado y actualizado en HuggingFace por su servidor) y evaluaremos las estrategias para reducir los costos de almacenamiento y consulta de embeddings.

### Prefacio: 쯇or qu칠 Qdrant?
Qdrant es un proyecto de c칩digo abierto para administrar bases de datos vectoriales y motores de b칰squeda de similitud. Permite muy f치cilmente crear colecciones de vectores y encontrar los m치s cercanos a un vector de consulta.

Es incre칤blemente r치pido y est치 dise침ado para escalar a miles de millones de vectores.

## Configuraci칩n del Entorno
Primero, configuraremos nuestro servidor de Qdrant localmente usando Docker. Hacerlo as칤 es robusto y apto para producci칩n, y resulta muy f치cil trasladarlo a la nube.

Aseg칰rate de tener Docker instalado y corriendo, luego ejecuta los siguientes comandos:

```bash
# 1. Descargar la 칰ltima imagen de Qdrant
docker pull qdrant/qdrant

# 2. Correr el contenedor de Qdrant
docker run -d -p 6333:6333 -p 6334:6334 \
    -v "$(pwd)/qdrant_storage:/qdrant/storage:z" \
    qdrant/qdrant
```
Esto iniciar치 Qdrant y mapear치 los puertos 6333 (REST API) y 6334 (gRPC) a tu m치quina local. Los datos se guardar치n en el directorio `qdrant_storage` en la carpeta actual.

Finalmente, usaremos `python-dotenv` para manejar nuestra API key de forma segura. Crea un archivo `.env` en la ra칤z de tu proyecto con el siguiente contenido:
```
GS_API_KEY="tu_clave_api_de_surus"
```

Luego, vamos a instalar las librer칤as que necesitaremos para interactuar con el servicio de Qdrant y la API de embeddings de surus:
```bash
# Crear un entorno virtual (opcional pero recomendado)
python -m venv venv
source venv/bin/activate  # En Linux/Mac
# venv\Scripts\activate  # En Windows
# Instalar dependencias
pip install qdrant-client datasets requests tqdm python-dotenv
```
## Cargando el Dataset y Configurando Clientes
El dataset del Bolet칤n Oficial lo descargaremos de [`marianbasti/boletin-oficial-argentina`](https://huggingface.co/datasets/marianbasti/boletin-oficial-argentina). En ese repositorio almacenamos el resultado de un script que raliza crawlings diarios de la web oficial, con el objetivo de tenerlo en un formato accesible y procesable.

En este paso tambi칠n configuraremos nuestros clientes para Qdrant y la API de embeddings de surus.

```python
import os
import requests
import qdrant_client
from datasets import load_dataset
from dotenv import load_dotenv
from tqdm.auto import tqdm

# Cargar variables de entorno
load_dotenv()

# Configurar cliente de Qdrant (conectado a nuestra instancia de Docker)
client = qdrant_client.QdrantClient(url="http://localhost:6333")

# Configurar API de surus
GS_API_KEY = os.getenv("GS_API_KEY")
API_URL = "https://api.surus.dev/functions/v1/embeddings"
headers = {"Authorization": "Bearer " + GS_API_KEY}
EMBEDDING_MODEL = "nomic-ai/nomic-embed-text-v2-moe"

# Cargar el dataset
print("Cargando dataset...")
dataset = load_dataset("marianbasti/boletin-oficial-argentina", split="train")
print(dataset)
```

Vamos a usar el modelo `nomic-ai/nomic-embed-text-v2-moe` de nomic-ai. Este es un modelo Matryoshka que permite generar embeddings de texto con una dimensi칩n variable.

Al momento de crear los embeddings, podremos especificar la dimensi칩n que deseamos y optimizar entre tama침o de almacenamiento y precisi칩n.

Este modelo tiene un l칤mite de largo de input de 512 tokens. por lo que:
1. Vamos a usar el endpoint `/tokenize` de surus para obtener los tokens
2. Agarraremos cada entrada de texto, y la chunkeremos en partes de 512 tokens.

## Funci칩n para Tokenizar y Chunkear Texto

El modelo de nomic tiene una ventana de contexto m치xima de 512 tokens. 

Para manejar esto, empezamos por definir funciones auxiliares para agarrar el texto y tokenizarlo usando la API de surus.

```python
def tokenize_text(text, model=EMBEDDING_MODEL):
    """
    Tokeniza un texto usando la API de surus.
    """
    tokenize_url = "https://api.surus.dev/functions/v1/tokenize"
    data = {
        "model": model,
        "input": text
    }
    headers_tokenize = headers.copy()
    headers_tokenize["Content-Type"] = "application/json"
    
    response = requests.post(tokenize_url, headers=headers_tokenize, json=data)
    response.raise_for_status()
    
    return response.json()['tokens']

def chunk_text_by_tokens(text, max_tokens=512, model=EMBEDDING_MODEL):
    """
    Divide un texto en chunks de m치ximo max_tokens tokens.
    """
    if not text or not text.strip():
        return []
    
    # Tokenizar el texto completo
    tokens = tokenize_text(text, model)
    
    # Si el texto ya es menor o igual al l칤mite, devolver como un solo chunk
    if len(tokens) <= max_tokens:
        return [text]
    
    # Dividir en chunks
    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i + max_tokens]
        # Reconstruir el texto desde los tokens
        # Nota: esto es una aproximaci칩n simple, en un caso real podr칤as
        # necesitar usar el detokenizer espec칤fico del modelo
        chunk_text = ' '.join(chunk_tokens)
        chunks.append(chunk_text)
    
    return chunks

# Prueba de la funci칩n de chunking
try:
    sample_text = "Este es un texto de prueba muy largo " * 100  # Texto repetitivo para testing
    chunks = chunk_text_by_tokens(sample_text, max_tokens=50)  # Usar 50 tokens para la prueba
    print(f"Texto dividido en {len(chunks)} chunks")
    print(f"Primer chunk: {chunks[0][:100]}...")
    if len(chunks) > 1:
        print(f"Segundo chunk: {chunks[1][:100]}...")
except Exception as e:
    print(f"Error en prueba de chunking: {e}")
```

## Funci칩n para Generar Embeddings
Ac치 actualizamos la funci칩n para manejar texto chunkeado y generar embeddings para cada chunk.

```python
def get_embeddings_for_chunks(text, model=EMBEDDING_MODEL, dimension_size=768, max_tokens=512):
    """
    Obtiene embeddings para un texto, dividi칠ndolo en chunks si es necesario.
    Retorna una lista de embeddings, uno por cada chunk.
    """
    chunks = chunk_text_by_tokens(text, max_tokens=max_tokens, model=model)
    
    if not chunks:
        return []
    
    # Generar embeddings para todos los chunks de una vez
    data = {
        "model": model,
        "input": chunks,
        "dimensions": dimension_size
    }
    headers_embed = headers.copy()
    headers_embed["Content-Type"] = "application/json"
    response = requests.post(API_URL, headers=headers_embed, json=data)
    response.raise_for_status()
    
    return response.json()['data'], chunks

def get_embeddings(texts, model=EMBEDDING_MODEL, dimension_size=768):
    """
    Funci칩n original mantenida para compatibilidad con textos ya cortos.
    """
    data = {
        "model": model,
        "input": texts,
        "dimensions": dimension_size
    }
    headers_embed = headers.copy()
    headers_embed["Content-Type"] = "application/json"
    response = requests.post(API_URL, headers=headers_embed, json=data)
    response.raise_for_status()
    
    return response.json()['data']

# Prueba de la funci칩n
try:
    sample_text = "Este es un texto de prueba para embeddings con chunking autom치tico."
    embeddings, chunks = get_embeddings_for_chunks(sample_text, dimension_size=768)
    print(f"Generados {len(embeddings)} embeddings para {len(chunks)} chunks")
    EMBEDDING_DIM = len(embeddings[0]['embedding'])
    print(f"Dimensi칩n de los embeddings: {EMBEDDING_DIM}")
except Exception as e:
    print(f"Error al obtener embeddings con chunking: {e}")
    print("Aseg칰rate de que tu GS_API_KEY es correcta y est치 en el archivo .env")
```

Prestemos especial atenci칩n al par치metro `dimension_size` en la funci칩n `get_embeddings`. Este nos permite especificar la dimensi칩n del embedding que queremos generar. Por defecto, usamos 768, pero podemos ajustarlo para reducir el tama침o del almacenamiento.

Es importante pasar este parametro EMBEDDING_DIM a la hora de crear la colecci칩n en Qdrant, es necesario definir el tama침o de dimensiones que se espera en la base de datos.

## Testeando Diferentes Dimensiones de Embeddings

Antes de crear la colecci칩n entera (y tomarnos el tiempo de indexar mas de 400 mil documentos!), vamos a comparar el desempe침o y almacenamiento usando tres dimensiones distintas: **768**, **512** y **256**.

Para esto, primero creamos tres colecciones en Qdrant, cada una con una dimensi칩n diferente.

Luego, indexaremos los mismos 1000 documentos aleatorios del dataset y observaremos el impacto en velocidad, almacenamiento y calidad de b칰squeda seg칰n la dimensi칩n elegida.

```python
import random

# Seleccionamos 1000 documentos aleatorios del dataset
MAX_TEST_DOCS = 1000
all_indices = list(range(len(dataset)))
random.shuffle(all_indices)
test_indices = all_indices[:MAX_TEST_DOCS]
test_texts = [dataset[i]['full_text'] for i in test_indices if dataset[i]['full_text'] and dataset[i]['full_text'].strip()]

# Definimos las dimensiones a testear
DIMENSIONS_TO_TEST = [768, 512, 256]
test_collection_names = [f"test-boletin-{dim}d" for dim in DIMENSIONS_TO_TEST]

from qdrant_client.models import VectorParams, Distance, PointStruct

for dim, col_name in zip(DIMENSIONS_TO_TEST, test_collection_names):
    print(f"\nCreando colecci칩n de test: {col_name} con dimensi칩n {dim}")
    try:
        client.create_collection(
            collection_name=col_name,
            vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
        )
    except Exception as e:
        print(f"Error al crear la colecci칩n {col_name}: {e}")

    # Indexamos los documentos en lotes
    BATCH_SIZE = 8
    for i in tqdm(range(0, len(test_texts), BATCH_SIZE), desc=f"Indexando en {col_name}"):
        batch = test_texts[i:i+BATCH_SIZE]
        embeddings = get_embeddings(batch, dimension_size=dim)
        points = [
            PointStruct(
                id=i + j,
                vector=emb['embedding'],
                payload={"text": text},
            )
            for j, (emb, text) in enumerate(zip(embeddings, batch))
        ]
        client.upsert(
            collection_name=col_name,
            points=points,
            wait=False
        )
    print(f"Indexaci칩n completada para {col_name}. Total: {len(test_texts)} documentos.")

```

Con esto, tendr치s tres colecciones de prueba para comparar c칩mo afecta la dimensionalidad al almacenamiento y la b칰squeda.

## Evaluando Almacenamiento, Velocidad y Resultados de B칰squeda

Ahora vamos a comparar las colecciones creadas en tres aspectos clave:

1. **Tama침o de almacenamiento:** Consultamos el tama침o en disco de cada colecci칩n.
2. **Velocidad de b칰squeda:** Medimos el tiempo que tarda una b칰squeda sem치ntica en cada colecci칩n.
3. **Calidad de resultados:** Imprimimos los resultados de una misma consulta en las tres colecciones para comparar la calidad de los resultados.

```python
import time

def get_collection_disk_size(collection_name):
    # Qdrant expone el tama침o en disco por colecci칩n en la API REST
    # Usamos el endpoint /collections/{collection_name}
    resp = requests.get(f"http://localhost:6333/collections/{collection_name}")
    resp.raise_for_status()
    info = resp.json()
    size_bytes = info['result']['status']['disk_data_size']
    size_mb = size_bytes / (1024 * 1024)
    return size_mb

def medir_busqueda_y_resultados(query, top_k=3):
    for col_name in test_collection_names:
        print(f"\nColecci칩n: {col_name}")
        # Tama침o en disco
        try:
            size_mb = get_collection_disk_size(col_name)
            print(f"Tama침o en disco: {size_mb:.2f} MB")
        except Exception as e:
            print(f"No se pudo obtener el tama침o en disco: {e}")

        # Medir tiempo de b칰squeda
        t0 = time.time()
        query_embedding = get_embeddings([query], dimension_size=int(col_name.split('-')[-1][:-1]))[0]
        resultados = client.search(
            collection_name=col_name,
            query_vector=query_embedding,
            limit=top_k,
            with_payload=True
        )
        t1 = time.time()
        print(f"Tiempo de b칰squeda: {t1-t0:.3f} segundos")

        # Imprimir resultados
        for i, hit in enumerate(resultados):
            print(f"--- Resultado {i+1} (Score: {hit.score:.4f}) ---")
            print(hit.payload['text'][:300].replace('\n', ' ') + "...")
        print("\n"+"="*60)

# Ejemplo de comparaci칩n
consulta = "쯈u칠 dice la ley sobre suspensiones de personal en una crisis empresarial?"
medir_busqueda_y_resultados(consulta)
```

Con este an치lisis, pod칠s observar el impacto real de la dimensionalidad en almacenamiento, velocidad y calidad de resultados.

## Creando la Colecci칩n en Qdrant
Una vez que hayamos decidido que dimensi칩n de vectores nos convence, vamos a crear la colecci칩n completa en Qdrant.

```python
from qdrant_client.models import VectorParams, Distance

collection_name = "boletin-oficial-argentina"

try:
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE), # Ac치 le decimos la dimensi칩n de los vectores y el tipo de distancia a usar para la b칰squeda de similitud
    )
except Exception as e:
    print(f"Error al crear la colecci칩n: {e}")
```

## Indexando los Documentos
Los pasos que seguimos son:
1. Agarramos nuestro dataset, iteramos creando chunks de m치ximo 512 tokens
2. Generamos embeddings para cada chunk con la API de surus y
3. Los agregamos a la colecci칩n de Qdrant.

```python
from qdrant_client.models import PointStruct

BATCH_SIZE = 2  # Reducido porque ahora procesamos chunks
MAX_TOKENS_PER_CHUNK = 512

total_docs = len(dataset)
print(f"Indexando {total_docs} documentos en lotes de {BATCH_SIZE}...")

point_id = 0  # Contador global para IDs 칰nicos

for i in tqdm(range(0, total_docs, BATCH_SIZE)):
    batch_data = dataset[i:i+BATCH_SIZE]
    
    # Procesar cada documento del batch
    points_to_insert = []
    
    for doc_idx, doc in enumerate(batch_data):
        text = doc['full_text']
        
        # Filtrar textos vac칤os
        if not text or not text.strip():
            continue
        
        try:
            # Generar embeddings con chunking autom치tico
            embeddings, chunks = get_embeddings_for_chunks(
                text, 
                dimension_size=EMBEDDING_DIM, 
                max_tokens=MAX_TOKENS_PER_CHUNK
            )
            
            # Crear un punto por cada chunk
            for chunk_idx, (embedding, chunk_text) in enumerate(zip(embeddings, chunks)):
                point = PointStruct(
                    id=point_id,
                    vector=embedding['embedding'],
                    payload={
                        "text": chunk_text,
                        "document_id": i + doc_idx,  # ID del documento original
                        "chunk_id": chunk_idx,       # ID del chunk dentro del documento
                        "total_chunks": len(chunks)  # Total de chunks del documento
                    },
                )
                points_to_insert.append(point)
                point_id += 1
                
        except Exception as e:
            print(f"Error procesando documento {i + doc_idx}: {e}")
            continue
    
    # Subir todos los puntos del batch a Qdrant
    if points_to_insert:
        client.upsert(
            collection_name=collection_name,
            points=points_to_insert,
            wait=False
        )

print("\nIndexaci칩n completada.")
collection_info = client.get_collection(collection_name=collection_name)
print(f"Total de vectores en la colecci칩n: {collection_info.vectors_count}")
```

Ten칠 paciencia! Este proceso puede llevar mucho tiempo.

Una vez que termina, ya est치! Tenemos una base de datos masiva con todos nuestros documentos indexados en Qdrant, listos para realizar b칰squedas sem치nticas.

Es ac치 donde observaremos cu치nto almacenamiento estamos usando y c칩mo podemos optimizarlo. Esto escala linealmente con las dimensiones, as칤 que si las reducimos por ejemplo, a la mitad, podemos reducir el costo de almacenamiento a la mitad.

## Buscando en la Base de Datos Legal
Con nuestros documentos indexados, podemos realizar b칰squedas sem치nticas.

```python
def buscar_documentos(query, top_k=5):
    """
    Busca documentos en Qdrant basados en una consulta de texto.
    """
    # 1. Obtener el embedding para la consulta
    query_embedding = get_embeddings([query])[0]
    
    # 2. Realizar la b칰squeda en Qdrant
    search_result = client.search(
        collection_name=collection_name,
        query_vector=query_embedding,
        limit=top_k,
        with_payload=True # Para que nos devuelva el texto del documento
    )
    
    return search_result

# Ejemplo de b칰squeda
pregunta = "쯈u칠 dice la ley sobre suspensiones de personal en una crisis empresarial?"
resultados = buscar_documentos(pregunta)

print(f"\n游댌 Resultados para la b칰squeda: '{pregunta}'\n")
for i, hit in enumerate(resultados):
    print(f"--- Resultado {i+1} (Score: {hit.score:.4f}) ---")
    # Mostramos los primeros 500 caracteres del texto
    print(hit.payload['text'][:500] + "...")
    print("\n")
```

## 쮺u치nto cuesta almacenar estos vectores?

Cuando querramos mover nuestro proyecto a producci칩n, es importante tener en cuenta el costo de almacenamiento de los embeddings.
Qdrant ofrece una calculadora de precios en su [sitio web](https://cloud.qdrant.io/calculator) que nos permite estimar el costo mensual seg칰n el tama침o de los vectores y la cantidad de datos.

## Conclusi칩n
Construiste un motor de b칰squeda sem치ntica para buscar a lo largo y ancho de cientos de miles de documentos legales argentinos. Hemos visto c칩mo:
1.  Configurar Qdrant con Docker para un entorno persistente.
2.  Cargar un dataset de textos legales.
3.  Crear una colecci칩n en Qdrant optimizada para b칰squeda de similitud.
4.  Indexar miles de documentos en lotes, generando embeddings sobre la marcha con la API de surus.
5.  Realizar b칰squedas sem치nticas para encontrar documentos relevantes a una pregunta en lenguaje natural.

Este es un pilar fundamental para construir aplicaciones de IA Legal m치s complejas.

## Pr칩ximos Pasos
- **RAG (Retrieval-Augmented Generation):** Combina este sistema de b칰squeda con un LLM (como los disponibles en surus.dev) para generar respuestas directas a las preguntas, en lugar de solo devolver documentos.
- **Metadatos y Filtros:** Enriquece los `payloads` en Qdrant con metadatos (fechas, tipo de norma, etc.) para permitir b칰squedas filtradas, combinando la b칰squeda sem치ntica con filtros exactos.
